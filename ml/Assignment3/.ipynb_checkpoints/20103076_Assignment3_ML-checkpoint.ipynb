{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8f7ff4",
   "metadata": {},
   "source": [
    "# ML assignment 3\n",
    "+ Name: Harshpreet Singh Johar\n",
    "+ SID: 20103076\n",
    "+ CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e90916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d6fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23f74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"data/train.csv\" , delimiter=\",\")\n",
    "testing = pd.read_csv(\"data/test.csv\" , delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62616a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = training.iloc[: , 0:79] # independent\n",
    "# dependent\n",
    "Y = training.iloc[: , 80]\n",
    "Y = Y.values\n",
    "Y = Y.reshape(-1,1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0650393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling NA for X with mode\n",
    "for i in X.columns : \n",
    "    X[i].fillna(X[i].mode().iloc[0] , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720cc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(X['Id'][0])\n",
    "ohe = OneHotEncoder()\n",
    "for i in X.columns :\n",
    "    x = type(X[i][0])\n",
    "    y = type(\"str\")\n",
    "    if x == y :\n",
    "        # One Hot Encoding Column\n",
    "        transformed = ohe.fit_transform(X[[i]])\n",
    "        X[ohe.categories_[0]] = transformed.toarray()\n",
    "        X.drop(i,axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1af632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X.columns :\n",
    "    x = type(X[i][0])\n",
    "    y = type(\"str\")\n",
    "    if x == y :\n",
    "        transformed = ohe.fit_transform(X[[i]])\n",
    "        X[ohe.categories_[0]] = transformed.toarray()\n",
    "        X.drop(i,axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c861a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "Y = scaler.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0592171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34727322]\n",
      " [ 0.00728832]\n",
      " [ 0.53615372]\n",
      " ...\n",
      " [ 1.07761115]\n",
      " [-0.48852299]\n",
      " [-0.42084081]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.730865</td>\n",
       "      <td>0.073375</td>\n",
       "      <td>-0.146189</td>\n",
       "      <td>-0.207142</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>1.050994</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.514104</td>\n",
       "      <td>0.575425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.728492</td>\n",
       "      <td>-0.872563</td>\n",
       "      <td>0.524992</td>\n",
       "      <td>-0.091886</td>\n",
       "      <td>-0.071836</td>\n",
       "      <td>2.179628</td>\n",
       "      <td>0.156734</td>\n",
       "      <td>-0.429577</td>\n",
       "      <td>-0.570750</td>\n",
       "      <td>1.171992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.726120</td>\n",
       "      <td>0.073375</td>\n",
       "      <td>-0.011953</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>0.984752</td>\n",
       "      <td>0.830215</td>\n",
       "      <td>0.325915</td>\n",
       "      <td>0.092907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.723747</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>-0.369915</td>\n",
       "      <td>-0.096897</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>-1.863632</td>\n",
       "      <td>-0.720298</td>\n",
       "      <td>-0.570750</td>\n",
       "      <td>-0.499274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.721374</td>\n",
       "      <td>0.073375</td>\n",
       "      <td>0.703973</td>\n",
       "      <td>0.375148</td>\n",
       "      <td>1.374795</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>0.951632</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>1.366489</td>\n",
       "      <td>0.463568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1.721374</td>\n",
       "      <td>0.073375</td>\n",
       "      <td>-0.280425</td>\n",
       "      <td>-0.260560</td>\n",
       "      <td>-0.071836</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>0.918511</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>-0.570750</td>\n",
       "      <td>-0.973018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1.723747</td>\n",
       "      <td>-0.872563</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.266407</td>\n",
       "      <td>-0.071836</td>\n",
       "      <td>0.381743</td>\n",
       "      <td>0.222975</td>\n",
       "      <td>0.151865</td>\n",
       "      <td>0.087911</td>\n",
       "      <td>0.759659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1.726120</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>-0.101443</td>\n",
       "      <td>-0.147810</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>3.078570</td>\n",
       "      <td>-1.002492</td>\n",
       "      <td>1.024029</td>\n",
       "      <td>-0.570750</td>\n",
       "      <td>-0.369871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1.728492</td>\n",
       "      <td>-0.872563</td>\n",
       "      <td>-0.011953</td>\n",
       "      <td>-0.080160</td>\n",
       "      <td>-0.795151</td>\n",
       "      <td>0.381743</td>\n",
       "      <td>-0.704406</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>-0.570750</td>\n",
       "      <td>-0.865548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1.730865</td>\n",
       "      <td>-0.872563</td>\n",
       "      <td>0.301265</td>\n",
       "      <td>-0.058112</td>\n",
       "      <td>-0.795151</td>\n",
       "      <td>0.381743</td>\n",
       "      <td>-0.207594</td>\n",
       "      <td>-0.962566</td>\n",
       "      <td>-0.570750</td>\n",
       "      <td>0.847389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02618</td>\n",
       "      <td>-0.1742</td>\n",
       "      <td>-0.052414</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.078757</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.058621</td>\n",
       "      <td>-0.301962</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>0.390293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 205 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -1.730865  0.073375 -0.146189 -0.207142  0.651479 -0.517200  1.050994   \n",
       "1    -1.728492 -0.872563  0.524992 -0.091886 -0.071836  2.179628  0.156734   \n",
       "2    -1.726120  0.073375 -0.011953  0.073480  0.651479 -0.517200  0.984752   \n",
       "3    -1.723747  0.309859 -0.369915 -0.096897  0.651479 -0.517200 -1.863632   \n",
       "4    -1.721374  0.073375  0.703973  0.375148  1.374795 -0.517200  0.951632   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1455  1.721374  0.073375 -0.280425 -0.260560 -0.071836 -0.517200  0.918511   \n",
       "1456  1.723747 -0.872563  0.748718  0.266407 -0.071836  0.381743  0.222975   \n",
       "1457  1.726120  0.309859 -0.101443 -0.147810  0.651479  3.078570 -1.002492   \n",
       "1458  1.728492 -0.872563 -0.011953 -0.080160 -0.795151  0.381743 -0.704406   \n",
       "1459  1.730865 -0.872563  0.301265 -0.058112 -0.795151  0.381743 -0.207594   \n",
       "\n",
       "           7         8         9    ...      195     196       197       198  \\\n",
       "0     0.878668  0.514104  0.575425  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "1    -0.429577 -0.570750  1.171992  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "2     0.830215  0.325915  0.092907  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "3    -0.720298 -0.570750 -0.499274  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "4     0.733308  1.366489  0.463568  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "...        ...       ...       ...  ...      ...     ...       ...       ...   \n",
       "1455  0.733308 -0.570750 -0.973018  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "1456  0.151865  0.087911  0.759659  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "1457  1.024029 -0.570750 -0.369871  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "1458  0.539493 -0.570750 -0.865548  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "1459 -0.962566 -0.570750  0.847389  ... -0.02618 -0.1742 -0.052414 -0.037037   \n",
       "\n",
       "           199       200       201       202       203       204  \n",
       "0    -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "1    -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "2    -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "3    -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "4    -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "1455 -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "1456 -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "1457 -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "1458 -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "1459 -0.078757 -0.058621 -0.058621 -0.301962 -0.045376  0.390293  \n",
       "\n",
       "[1460 rows x 205 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y)\n",
    "pd.DataFrame(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f552552",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test ,y_train , y_test = train_test_split(X,Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c61c72",
   "metadata": {},
   "source": [
    "# Using the Mathematical Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a1ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def fit(x,y) :\n",
    "    num = (x*y).mean() - x.mean() * y.mean() \n",
    "    den = (x**2).mean() - (x.mean())**2\n",
    "    m = num / den\n",
    "    c = y.mean() - m * x.mean()\n",
    "    return m,c\n",
    "\n",
    "def predict(x , m , c ) :\n",
    "    return m * x + c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6e92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance measures\n",
    "\n",
    "def score(y_true , y_pred) :\n",
    "    u = ((y_true - y_pred)**2).sum()\n",
    "    v = ((y_true - y_true.mean())**2).sum()\n",
    "    return 1 - u/v\n",
    "def cost(m,x,y,c) :\n",
    "    return ((y - m*x - c)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14c1a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044806740771907404 0.0031004255122607446\n"
     ]
    }
   ],
   "source": [
    "m , c = fit(x_train , y_train)\n",
    "print(m,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "376e9bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score -203.66860179432945\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(x_test , m , c)\n",
    "sc = score(y_test , y_pred) \n",
    "print(\"Score\" , sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de930dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost 0.9168412165012384\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost\" , cost(m, x_test , y_test , c)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4813",
   "metadata": {},
   "source": [
    "# Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a234f32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "196b7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7745e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = clf.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56ac679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clf.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ee7fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(X,Y, learning_rate , m , c) :\n",
    "    m_slope = np.zeros(len(X[0]))\n",
    "    c_slope = 0 \n",
    "    M=len(X)\n",
    "    for i in range(M) :\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        submis = 0 \n",
    "        for j in range(len(m_slope)):\n",
    "            submis += m[j]*x[j] \n",
    "        for j in range(len(m_slope)) :\n",
    "            m_slope[j] += (-2/M)*(y-submis-c)*x[j]\n",
    "        c_slope += (-2/M)*(y-submis-c)\n",
    "    new_m = m - learning_rate*m_slope\n",
    "    new_c = c - learning_rate*c_slope\n",
    "    print(cost(new_m, X, Y,new_c))\n",
    "    return new_m , new_c ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa1d6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(X , Y, learning_rate , nums_iteration ) :\n",
    "    m = np.zeros(len(X[0]))\n",
    "    c = 0\n",
    "    for i in range(nums_iteration) :\n",
    "        m , c = step_gradient(X, Y , learning_rate , m , c )\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28c0ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    learning_rate = 0.000001 \n",
    "    nums_iteration = 10\n",
    "    m , c = gd(x_train , y_train, learning_rate, nums_iteration)\n",
    "    return m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "797ed541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0271975593340237\n",
      "1.0271973749017094\n",
      "1.0271971904740884\n",
      "1.0271970060511597\n",
      "1.0271968216329246\n",
      "1.0271966372193815\n",
      "1.0271964528105308\n",
      "1.0271962684063727\n",
      "1.0271960840069068\n",
      "1.027195899612133\n"
     ]
    }
   ],
   "source": [
    "m , c = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf2ef10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-204.01773871992307\n",
      "0.9184052234900015\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(x_test ,m ,c )\n",
    "sc = score(y_test ,y_pred)\n",
    "print(sc)\n",
    "cst = cost(m,x_test,y_test,c)\n",
    "print(cst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
